# 简介

这个代码库是使用`Python +
Numpy`对MNIST数据集训练的多层神经网络的简单实现，主要包含六部分，在任务一中对多层神经网络进行了实现，在任务二中对于不同层的神经网络进行了分析比较，
在后面的任务中，分别探索了对于使用不同种梯度下降优化算法、参数初始化方法、学习率优化算法和正则化方法对于神经网络训练的影响

## 运行说明

在仓库根目录下运行类似`python ./taskX/taskX.py`命令即可，运行结果保存在`./taskX/image/`
文件夹中，模型训练参数及绘制图像所需的参数保存在`./taskX/result`文件夹中

## 任务说明

### Task 1

一个纯净的三层神经网络，不使用任何优化算法和学习率优化算法

### Task 2

在三层神经元的基础上对比了增加一个隐藏层和减少一个层对于训练结果的影响，分析得到了两层神经网络和四层神经网络的训练结果，
可以看到在训练相同的Epoch，两层神经网络的训练结果很不稳定，训练结果波动很大

### Task 3

分析了使用三种不同的梯度下降优化算法对于神经网络训练的结果，分别是Mini-Batch-Gradient-Decent、Batch-Gradient-Decent和Stochastic-Gradient-Decent，
可以看出

- 使用BGD得到的训练网络非常的平滑，训练结果十分稳定，
- 相对来说，使用其他两种得到的训练曲线波动就比较大，结果很不稳定
- 并且从训练时间来看，使用SGD每一个训练的Epoch十分耗时，并且很难取得很好的结果

### Task 4

分析了四种不同的参数初始化方法，分别是零初始化、随机初始化、Xavier初始化和He初始化
。四种初始化方法中可以很明显的看出来零初始化取得的效果最差，其他几种方法均取得了相差不大的结果

### Task 5

分析了使用四种不同的学习率优化算法取得的训练结果，分别是固定学习率、Momentum、RMSProp和Adam，可以看到对于固定学习率来说可以在很短的训练时间内取得比较好的训练结果

### Task 6

分析对比了使用三种不同正则化方法取得的结果，分别是无正则化，L1正则化和L2正则化
